{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'pytorch tensor cheatsheet'\n",
    "description: 'pytorch tensor cheatsheet'\n",
    "author: 'janf'\n",
    "date: '2023-09-09'\n",
    "date-format: iso\n",
    "categories: [cheatsheet]\n",
    "toc: true\n",
    "execute: \n",
    "  enabled: false\n",
    "format:\n",
    "  html:\n",
    "    code-copy: true\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of model, as well as the model's parameters. Tensors are similar to NumPy's arrays, expect that tensors can run on GPU or other hardware.\n",
    "\n",
    "[PyTorch Tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "\n",
    "[Introduction to PyTorch Tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tensors\n",
    "\n",
    "Ways to create a tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Method\n",
    "\n",
    "Creating an empty tensor. <b>torch.empty()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5766e-19, 1.0256e-08, 2.5783e-09, 3.0441e+12],\n",
      "        [4.3353e-08, 1.7188e-04, 6.7510e-07, 1.2611e+16],\n",
      "        [2.1707e-18, 7.0952e+22, 1.7748e+28, 1.8176e+31]])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor with 2-dimensions, 3 rows and 4 columns.\n",
    "x = torch.empty(3,4)\n",
    "# by default tenors are 32-bit gloating point numbers.\n",
    "# torch.empty() allocates memory for the tensor but does not initialize it with any values\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tensor full of zeros. <b>torch.zeros()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor with 2-dimensions, 2 rows and 3 columns.\n",
    "x = torch.zeros(2,3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tensor full of ones. <b>torch.ones()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "# creating a tensor with 2-dimensions, 3 rows and 1 column.\n",
    "x = torch.ones(3,1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a tenor full of random values. <b>torch.rand()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5305, 0.9925, 0.7754],\n",
      "        [0.9989, 0.3047, 0.9887]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(187)\n",
    "# creating a tensor with 2-dimension, 2 rows and 3 columns.\n",
    "random = torch.rand(2,3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tensor with specific data directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>torch.tensor()</b> is the way to create a tenors if you have a Python tuple or list. Is creates a copy of the data as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(([1,2,3],[4,5,6]))\n",
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tensor and seeding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>torch.manual_seed()</b> sets the random seed to fixes the random outputs.\n",
    "\n",
    "<b>torch.rand()</b> creates a random tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5305, 0.9925, 0.7754, 0.9989, 0.3047],\n",
      "        [0.9887, 0.3299, 0.2694, 0.5281, 0.8815],\n",
      "        [0.5275, 0.7802, 0.9964, 0.1060, 0.5047],\n",
      "        [0.6960, 0.1014, 0.8651, 0.9504, 0.7015],\n",
      "        [0.2917, 0.7787, 0.3808, 0.2624, 0.6519]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(187)\n",
    "random = torch.rand(5,5)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensor with the shape like\n",
    "\n",
    "Creating a tensor having the same number dimensions and the same number of cells in each dimension.\n",
    "- <b>torch.empty_like()</b>\n",
    "- <b>torch.zeros_like()</b>\n",
    "- <b>torch.ones_like()</b>\n",
    "- <b>torch.rand_like()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0000e+00, 6.8664e-44, 2.7002e-06],\n",
      "         [2.6965e+23, 3.3429e+21, 1.0489e-08]],\n",
      "\n",
      "        [[1.6505e-07, 1.7664e-04, 4.1532e-08],\n",
      "         [5.3928e-05, 1.6597e-07, 5.3130e-08]]])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,2,3)\n",
    "print(x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology about tensors and thier number of dimensions\n",
    "\n",
    "- 0-dimensional tensor is called a scalar\n",
    "- 1-dimensional tensor is called a vector\n",
    "- 2-dimensional tensor is called a matrix\n",
    "- 3-dimensional or more tensor is called a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar (0-dim)\n",
      "tensor([1.])\n",
      "\n",
      "\n",
      "vector 1-dim\n",
      "tensor([[1., 1., 1., 1.]])\n",
      "\n",
      "\n",
      "matrix 2-dim\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "\n",
      "tensor 3-dim or more\n",
      "tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "d0 = torch.ones(1)\n",
    "print('scalar (0-dim)')\n",
    "print(d0)\n",
    "print('\\n')\n",
    "\n",
    "d1 = torch.ones(1,4)\n",
    "print('vector 1-dim')\n",
    "print(d1)\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "d2 = torch.ones(3,3)\n",
    "print('matrix 2-dim')\n",
    "print(d2)\n",
    "print('\\n')\n",
    "\n",
    "d3 = torch.ones(4,4,4)\n",
    "print('tensor 3-dim or more')\n",
    "print(d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>.shape</b> property gives list of the extant of each dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(2,1)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor data types\n",
    "\n",
    "One way to set the data type of a tensor is with an optional argument at creation.\n",
    "\n",
    "You can see the specified dtype by printing the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3), dtype=torch.int16)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to set the datatype is with the **.to()** method. It converts a float64 and creates a int32 tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3), dtype=torch.float64)\n",
    "print(a)\n",
    "\n",
    "b = a.to(torch.int32)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch datatypes:\n",
    "\n",
    "- torch.bool\n",
    "- torch.int8\n",
    "- torch.uint8\n",
    "- torch.int16\n",
    "- torch.int32\n",
    "- torch.int64\n",
    "- torch.half\n",
    "- torch.float\n",
    "- torch.double\n",
    "- torch.bfloat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math & Logic\n",
    "\n",
    "Basic arithmetic with tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "twos = torch.ones(2,2) * 2\n",
    "print(twos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arithmetic operations between tensors and scalars, such as addition, subtraction, multiplication, division, and exponentiation are distributed over every element of the tensor.\n",
    "\n",
    "Operation between tow tensors also behave intuitively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.ones(2,2)\n",
    "print(ones + twos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors of dissimilar shape will throw a run-time error when calculated via binary operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb Cell 37\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#X51sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#X51sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m,\u001b[39m4\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#X51sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(a \u001b[39m*\u001b[39;49m b)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# The following throws a run-time error. This is intentional.\n",
    "a = torch.rand(2,3)\n",
    "b = torch.rand(3,4)\n",
    "\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More math-operations:\n",
    "\n",
    "[More Pytorch Math-Operations](https://pytorch.org/docs/stable/torch.html#math-operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Broadcasting\n",
    "\n",
    "The exception to the same-shape rule is tensor broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6098, 0.3175, 0.3386, 0.4934],\n",
      "        [0.9187, 0.8510, 0.7339, 0.3254]])\n",
      "tensor([[1.2195, 0.6349, 0.6772, 0.9867],\n",
      "        [1.8375, 1.7020, 1.4677, 0.6508]])\n"
     ]
    }
   ],
   "source": [
    "rand = torch.rand(2,4)\n",
    "doubled = rand * (torch.ones(1,4)*2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it we get to multiply a 2x4 tensor by a 1x4 tensor?\n",
    "\n",
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example above, the one-row, four-column tensor is multiplied by both rows of the two-row, four-column tensor.\n",
    "\n",
    "broadcasting examples:\n",
    "\n",
    "![Pytorch Broadcasting](broadcasting.jpg)\n",
    "\n",
    "This is an important operation in Deep Learning. The common example is multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, and returning a tensor of identical shape - just like (2,4) * (1,4) example above returned a tensor of shape (2,4).\n",
    "\n",
    "The rules of broadcasting are:\n",
    "\n",
    "- Each tensor must have at least one dimension - no empty tensors.\n",
    "- Comparing the dimension sizes of the two tensors, going form last to first:\n",
    "    - Each dimension must be equal of\n",
    "    - One of the dimension must be of size 1, or\n",
    "    - Dimension does not exist in one of the tensors\n",
    "\n",
    "Tensors of identical shape, of course are trivially \"broadcastable\", as you saw earlier.\n",
    "\n",
    "Here are some examples of situation that honor the above rules and allow broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,3,2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6381, 0.9161],\n",
      "         [0.5682, 0.7014],\n",
      "         [0.7000, 0.8772]],\n",
      "\n",
      "        [[0.6381, 0.9161],\n",
      "         [0.5682, 0.7014],\n",
      "         [0.7000, 0.8772]],\n",
      "\n",
      "        [[0.6381, 0.9161],\n",
      "         [0.5682, 0.7014],\n",
      "         [0.7000, 0.8772]],\n",
      "\n",
      "        [[0.6381, 0.9161],\n",
      "         [0.5682, 0.7014],\n",
      "         [0.7000, 0.8772]]])\n"
     ]
    }
   ],
   "source": [
    "b = a * torch.rand(3,2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6071, 0.9969],\n",
      "         [0.6071, 0.9969],\n",
      "         [0.6071, 0.9969]],\n",
      "\n",
      "        [[0.6071, 0.9969],\n",
      "         [0.6071, 0.9969],\n",
      "         [0.6071, 0.9969]],\n",
      "\n",
      "        [[0.6071, 0.9969],\n",
      "         [0.6071, 0.9969],\n",
      "         [0.6071, 0.9969]],\n",
      "\n",
      "        [[0.6071, 0.9969],\n",
      "         [0.6071, 0.9969],\n",
      "         [0.6071, 0.9969]]])\n"
     ]
    }
   ],
   "source": [
    "d = a * torch.rand(1,2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb Cell 45\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(     \u001b[39m4\u001b[39m,  \u001b[39m3\u001b[39m,  \u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y101sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m b \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrand(     \u001b[39m4\u001b[39;49m,  \u001b[39m3\u001b[39;49m)  \u001b[39m# dimension must match last-to-first\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y101sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m c \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrand(     \u001b[39m2\u001b[39m,  \u001b[39m3\u001b[39m)  \u001b[39m# both 3rd & 2nd dims different\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y101sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m d \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m0\u001b[39m,)          \u001b[39m# cant broadcast with an empty tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a = torch.ones(     4,  3,  2)\n",
    "\n",
    "b = a * torch.rand(     4,  3)  # dimension must match last-to-first\n",
    "\n",
    "c = a * torch.rand(     2,  3)  # both 3rd & 2nd dims different\n",
    "\n",
    "d = a * torch.rand(0,)          # cant broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Altering Tensors in Place\n",
    "\n",
    "Most binary operations on tensors will return a third, new tensor. When we say c = a * b (where a and b are tensors), the new tensor c will occupy a region of memory distinct from the other tensors.\n",
    "\n",
    "There are times, though, that you may whish to alter a tensor in place - for example. if you're doing an element-wise computation where you can discard intermediate value. For this, most of the math function have a version with an underscore(_) that will alter a tensor in place.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))     # this operation creates a new tensor in memory\n",
    "print(a)                # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))    # note the underscore\n",
    "print(b)                # b has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arithmetic operations, here are function that behave similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.4583, 0.6130],\n",
      "        [0.6209, 0.9512]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.4583, 1.6130],\n",
      "        [1.6209, 1.9512]])\n",
      "tensor([[1.4583, 1.6130],\n",
      "        [1.6209, 1.9512]])\n",
      "tensor([[0.4583, 0.6130],\n",
      "        [0.6209, 0.9512]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.2100, 0.3757],\n",
      "        [0.3856, 0.9048]])\n",
      "tensor([[0.2100, 0.3757],\n",
      "        [0.3856, 0.9048]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.rand(2,2)\n",
    "\n",
    "print('Before: ')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these in-place arithmetic functions are methods an the torch.Tensor object, not attached to the torch module like many other functions (e.g. torch.sin()). As you can see from a.add_(b), the calling tensor is the one that gets changed in place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Tensors\n",
    "\n",
    "As with any object in Python, assigning a tensor to a variable makes the variable a label of the tensor, and does not copy it. For example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561   # we change a ...\n",
    "print(b)        # and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is if you want a separated copy of the data to work on? The clone() method is there for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a       # differen objects in  memory ...\n",
    "print(torch.eq(a,b))    # ... but still with the same contents\n",
    "\n",
    "a[0][1] = 561\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important thing to be aware of when using clone(). If your source tensor has autograd enabled then so will the clone. This will be covered more deeply in the video on autograd, but if you want the light version of the details, continue on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU\n",
    "\n",
    "To check whether a GPU is available we can use <b>torch.cuda.is_available()</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default new tensors are created on the CPU. You have to specify an optional device argument to use the tensor on a GPU.\n",
    "\n",
    "As an coding practice, specifying our devices everywhere with string constants is pretty fragile. You can create a device handle to pass it to you tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.2694, 0.5133],\n",
      "        [0.4625, 0.0572]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2,2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With <b>.to()</b> you can move a tensor from CPU to a GPU or vise versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2,2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors must be on the same device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb Cell 63\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y126sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensor_note.ipynb#Y126sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m z \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m y \u001b[39m# exception will be thrown\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2, device='cpu')\n",
    "y = torch.rand(2,2, device='cuda')\n",
    "\n",
    "z = x + y # exception will be thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensor Shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Shape by Squwwzing And Unsqueezing\n",
    "<b>unsqueeze()</b> method adds a dimension of extent 1. unsqueesze(1) adds it as a new zeroth dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 266, 266])\n",
      "torch.Size([1, 2, 266, 266])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,266,266)\n",
    "b = a.unsqueeze(0)\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we <b>squeeze()</b> a tensor, the dimensions of size 1 are removed. The elements of the original tensor are arranged with the remaining dimensions. For example, if the input tensor is of shape: (m×1×n×1) then the output tensor after squeeze will be of shape: (m×n)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5690, 0.0415],\n",
      "        [0.1794, 0.7428]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[0.5690, 0.0415],\n",
      "        [0.1794, 0.7428]])\n",
      "torch.Size([2, 2])\n",
      "tensor([[0.1640, 0.1609, 0.6381, 0.9161, 0.5682, 0.7014, 0.7000, 0.8772, 0.6071,\n",
      "         0.9969, 0.1627, 0.6761, 0.0416, 0.7503, 0.2614, 0.9380, 0.2171, 0.8036,\n",
      "         0.7253, 0.2340]])\n",
      "torch.Size([1, 20])\n",
      "tensor([0.1640, 0.1609, 0.6381, 0.9161, 0.5682, 0.7014, 0.7000, 0.8772, 0.6071,\n",
      "        0.9969, 0.1627, 0.6761, 0.0416, 0.7503, 0.2614, 0.9380, 0.2171, 0.8036,\n",
      "        0.7253, 0.2340])\n",
      "torch.Size([20])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "\n",
    "b = a.squeeze()\n",
    "print(b)\n",
    "print(b.shape)\n",
    "\n",
    "c = torch.rand(1,20)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze()\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping a Tensor without changing the rank\n",
    "\n",
    "Changing a tensor without changing the rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.]])\n",
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
      "tensor([[1., 1., 1., 1., 2., 2.],\n",
      "        [2., 2., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "print(t)\n",
    "\n",
    "d = t.reshape([1,12])\n",
    "print(d)\n",
    "\n",
    "d = t.reshape([2,6])\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using <b>reshape()</b> we can specify the row x column shape that we are seeking. All of the shapes have to account for the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "tensor([1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
      "torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "], dtype=torch.float32)\n",
    "print(t.shape)\n",
    "\n",
    "d = t.flatten()\n",
    "print(d)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch-Numpy Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2,3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch creates a tensor of the same shape and containing the same data as the NumPy array, going so far as to keep NumPys default 64bit float data type. The conversion can also go the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1050, 0.5877, 0.4583],\n",
      "        [0.6130, 0.6209, 0.9512]])\n",
      "[[0.10500968 0.5877479  0.4582944 ]\n",
      " [0.61297506 0.62093675 0.9512319 ]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2,3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know that these converted objects are using the same underlying memory as thier source objects, meaning that changes to one are reflected in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1,1] = 23\n",
    "print(pytorch_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
