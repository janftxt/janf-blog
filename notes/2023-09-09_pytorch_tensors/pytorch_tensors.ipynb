{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'pytorch tensors'\n",
    "description: 'pytorch tensors'\n",
    "author: 'janf'\n",
    "date: '2023-09-09'\n",
    "date-format: iso\n",
    "categories: [notes]\n",
    "toc: true\n",
    "execute: \n",
    "  enabled: false\n",
    "format:\n",
    "  html:\n",
    "    code-copy: true\n",
    "draft: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Tensors\n",
    "\n",
    "Tensors are a specialized data structure that are very similar to arrays and matrices. In PyTorch, we use tensors to encode the inputs and outputs of model, as well as the model's parameters. Tensors are similar to NumPy's arrays, expect that tensors can run on GPU or other hardware.\n",
    "\n",
    "[PyTorch Tensor](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)\n",
    "\n",
    "[Introduction to PyTorch Tensors](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PyTorch Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: flase\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factory Method to create Tensor\n",
    "The simplest way to create a tensor is with the torch.empty() call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "tensor([[1.2268e-35, 0.0000e+00, 3.0829e-44, 0.0000e+00],\n",
      "        [       nan, 0.0000e+00, 1.8314e+25, 6.9768e+22],\n",
      "        [8.5305e+02, 2.6778e+20, 3.0866e+29, 1.0170e+31]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(3,4)\n",
    "print(type(x))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- this creates a tensor using on of the numerous factory methods attached to the **torch** module.\n",
    "- the tensor itself is 2-dimensional, having 3 rows and 4 columns.\n",
    "- the type of the object returned is **torch.Tensor**, which is an alias for **torch.FloatTensor**, by default PyTorch tensors are 32-bit floating point numbers.\n",
    "- there are some random-looking values in the tensor. The **torch.empty()** call allocates memory for the tensor, but does not initialize it with any values -  so what your're seeing is whatever was in memory at the time of allocation.\n",
    "\n",
    "If you want to initialize the tensor with some vales. Common cases are all zeros, all ones, or random values. The torch module provides factory methods for alle of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[0.3699, 0.5704, 0.4876],\n",
      "        [0.3391, 0.1535, 0.0455]])\n"
     ]
    }
   ],
   "source": [
    "# create a tensor full of zeros\n",
    "zeros = torch.zeros(2,3)\n",
    "print(zeros)\n",
    "\n",
    "# create a tensor full of ones\n",
    "ones = torch.ones(2,3)\n",
    "print(ones)\n",
    "\n",
    "# create a tensor full of random values\n",
    "torch.manual_seed(1779)\n",
    "random = torch.rand(2,3)\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terminology about tensors and thier number of dimensions\n",
    "\n",
    "- You will sometimes see a **1-dimensional tensor** called a **vector**.\n",
    "- A **2-dimensional tensor** is often referred as a **matrix**.\n",
    "- Anything with **more than two dimensions** is generally just called a **tensor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Tensor and Seeding\n",
    "\n",
    "Sometimes you want the same random values for reproducibility. Manually setting your random number generator's seed fixes the random outputs to get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2,3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2,3)\n",
    "print(random2)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2,3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2,3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should see above is that **random1** and **random3** carry identical values, as do **random2** and **random4**. Manually resetting the seed gives the same results when you compute the things again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Shapes\n",
    "\n",
    "Often when you are performing operations on two or more tensors, they will need to be of the same shape - that is, having the same number of dimensions and the same number of cells in each dimension. For that, we have the **torch.*_like()** methodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1.3368e-35, 0.0000e+00, 1.2282e-35],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1.1759e-09, 4.5806e-41, 1.2202e-35],\n",
      "         [0.0000e+00, 4.4842e-44, 0.0000e+00]],\n",
      "\n",
      "        [[1.1210e-43, 0.0000e+00, 1.2240e-35],\n",
      "         [0.0000e+00, 1.4013e-45, 0.0000e+00]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0., 0., 0.],\n",
      "         [0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.],\n",
      "         [0., 0., 0.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[0.6128, 0.1519, 0.0453],\n",
      "         [0.5035, 0.9978, 0.3884]],\n",
      "\n",
      "        [[0.6929, 0.1703, 0.1384],\n",
      "         [0.4759, 0.7481, 0.0361]]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(2,2,3)\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "empty_like_x = torch.empty_like(x)\n",
    "print(empty_like_x.shape)\n",
    "print(empty_like_x)\n",
    "\n",
    "zeros_like_x = torch.zeros_like(x)\n",
    "print(zeros_like_x.shape)\n",
    "print(zeros_like_x)\n",
    "\n",
    "ones_like_x = torch.ones_like(x)\n",
    "print(ones_like_x.shape)\n",
    "print(ones_like_x)\n",
    "\n",
    "rand_like_x = torch.rand_like(x)\n",
    "print(rand_like_x.shape)\n",
    "print(rand_like_x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first new thing in the code cell above is the use of the **.shape** property on a tensor. This property contains a list of the extent of each dimension of a tensor - in our case, **x** is a three-dimensional tensor with shape 2x2x3. Then we created new tensor's with **.empty_like(), .zeros_like(), .ones_like(), .rand_like()** methods. With **.shape** we can verify that **x** has the same size as our new tensors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Tensor with specific data directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([[3.1410, 2.7890],\n",
      "        [1.2340, 4.9230]])\n",
      "torch.Size([10])\n",
      "tensor([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "torch.Size([2, 3])\n",
      "tensor([[2, 4, 6],\n",
      "        [3, 6, 9]])\n"
     ]
    }
   ],
   "source": [
    "some_constants = torch.tensor([[3.141, 2.789], [1.234, 4.923]])\n",
    "print(some_constants.shape)\n",
    "print(some_constants)\n",
    "\n",
    "some_intergers = torch.tensor((2,3,4,5,6,7,8,9,10,11))\n",
    "print(some_intergers.shape)\n",
    "print(some_intergers)\n",
    "\n",
    "more_intergers = torch.tensor(((2,4,6), [3,6,9]))\n",
    "print(more_intergers.shape)\n",
    "print(more_intergers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **torch.tensor()** is the most straightforward way to create a tensor if your already have data in a Python tuple or list. Nesting will result in multi-dimensional tensor.\n",
    "\n",
    "Note: **torch.tensor()** creates a copy of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Tensor Shapes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider tensor shapes as the number of lists that a dimension holds. For instance, a tensor shaped (4,4,2) will have four elements, which all contain 4 elements, which in turn have 2 elements.\n",
    "\n",
    "1. The first holds 4 elements.\n",
    "2. The second holds 4 elements.\n",
    "3. The third dimension holds 2 elements.\n",
    "\n",
    "![torch.Size([4,4,2\"])](441.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2])\n",
      "tensor([[[1.1759e-09, 4.5806e-41],\n",
      "         [1.1759e-09, 4.5806e-41],\n",
      "         [0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00],\n",
      "         [1.8788e+31, 1.7220e+22],\n",
      "         [2.1715e-18, 4.2533e-05],\n",
      "         [1.3148e+22, 5.4412e-05]],\n",
      "\n",
      "        [[6.5555e-10, 3.3058e+21],\n",
      "         [3.2722e+21, 8.2730e+20],\n",
      "         [5.3134e-08, 1.7264e-07],\n",
      "         [3.2480e-09, 2.3052e-12]],\n",
      "\n",
      "        [[1.8788e+31, 7.9303e+34],\n",
      "         [6.1949e-04, 1.8590e+34],\n",
      "         [7.7767e+31, 7.1536e+22],\n",
      "         [3.3803e-18, 2.0552e+32]]])\n"
     ]
    }
   ],
   "source": [
    "tensor442 = torch.empty(4,4,2)\n",
    "print(tensor442.shape)\n",
    "print(tensor442)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structures in tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tensor:\n",
      "tensor([1.])\n",
      "create tensor:\n",
      "tensor([1.])\n",
      "get value 1:\n",
      "tensor(1.)\n",
      "shape:\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# [x]\n",
    "# ----------------------- \n",
    "# scalar\n",
    "# 0D (zero dimension)\n",
    "print('create tensor:')\n",
    "d0 = torch.ones(1)\n",
    "print(d0)\n",
    "\n",
    "print('create tensor:')\n",
    "d0 = torch.tensor([1.])\n",
    "print(d0)\n",
    "\n",
    "print('get value 1:')\n",
    "print(d0[0])\n",
    "\n",
    "print('shape:')\n",
    "print(d0.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tensor:\n",
      "tensor([1., 1., 1.])\n",
      "create tensor:\n",
      "tensor([1., 2., 3.])\n",
      "get value 3:\n",
      "tensor(3.)\n",
      "shape:\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# [x]\n",
    "# [x]\n",
    "# [x]\n",
    "# -----------------------\n",
    "# vector\n",
    "# 1D (one dimension)\n",
    "print('create tensor:')\n",
    "d1 = torch.ones(3)\n",
    "print(d1)\n",
    "\n",
    "print('create tensor:')\n",
    "d1 = torch.tensor([1.,2.,3.])\n",
    "print(d1)\n",
    "\n",
    "print('get value 3:')\n",
    "print(d1[2])\n",
    "\n",
    "print('shape:')\n",
    "print(d1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tensor:\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "create tensor:\n",
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n",
      "get value 4:\n",
      "tensor(4.)\n",
      "shape:\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# [x x x]\n",
    "# [x x x]\n",
    "# [x x x]\n",
    "# -----------------------\n",
    "# matrix\n",
    "# 2D (two dimension)\n",
    "print('create tensor:')\n",
    "d2 = torch.ones(3,3)\n",
    "print(d2)\n",
    "\n",
    "print('create tensor:')\n",
    "d2 = torch.tensor([[1.,2.,3.],[4.,5.,6.],[7.,8.,9.]])\n",
    "print(d2)\n",
    "\n",
    "print('get value 4:')\n",
    "print(d2[1,0])\n",
    "\n",
    "print('shape:')\n",
    "print(d2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tensor:\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n",
      "create tensor:\n",
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[2., 2., 2.],\n",
      "         [2., 5., 2.],\n",
      "         [2., 2., 2.]],\n",
      "\n",
      "        [[3., 3., 3.],\n",
      "         [3., 3., 3.],\n",
      "         [3., 3., 3.]]])\n",
      "get value:\n",
      "tensor(5.)\n",
      "shape:\n",
      "torch.Size([3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# [x x x] [x x x] [x x x]\n",
    "# [x x x] [x x x] [x x x]\n",
    "# [x x x] [x x x] [x x x]\n",
    "# -----------------------\n",
    "# tensor\n",
    "# 3D (3 dimension)\n",
    "print('create tensor:')\n",
    "d3 = torch.ones(3,3,3)\n",
    "print(d3)\n",
    "\n",
    "print('create tensor:')\n",
    "d3 = torch.tensor([[[1.,1.,1.],[1.,1.,1.],[1.,1.,1.]],[[2.,2.,2.],[2.,5.,2.],[2.,2.,2.]],[[3.,3.,3.],[3.,3.,3.],[3.,3.,3.]]])\n",
    "print(d3)\n",
    "\n",
    "print('get value:')\n",
    "print(d3[1,1,1])\n",
    "\n",
    "print('shape:')\n",
    "print(d3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tensor:\n",
      "tensor([[[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1.],\n",
      "          [1., 1.]],\n",
      "\n",
      "         [[1., 1.],\n",
      "          [1., 1.]]]])\n"
     ]
    }
   ],
   "source": [
    "# dimension > 3\n",
    "# -----------------------\n",
    "# tensor\n",
    "# XD (x > 3 dimension)\n",
    "\n",
    "print('create tensor:')\n",
    "dx = torch.ones(4,2,2,2)\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the datatype of a tensor is possible a couple of ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[ 0.9956,  1.4148,  5.8364],\n",
      "        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n",
      "tensor([[ 0,  1,  5],\n",
      "        [11, 11, 11]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3), dtype=torch.int16)\n",
    "print(a)\n",
    "\n",
    "b = torch.rand((2,3), dtype=torch.float64) * 20\n",
    "print(b)\n",
    "\n",
    "c = b.to(torch.int32)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to set underlying data type of a tensor is with an optional argument at creation time. At \"a\" we set **dtype=torch.int16**. Printing **a** shows that the data is 1 rather than 1. (1 int, 1. float)\n",
    "\n",
    "Another thing to notice by printing out a tensor it also shows the specified dtype.\n",
    "\n",
    "Another way to set the datatype is with the **.to()** method. In the cell above, we create a random floating point tensor **b** and den converted b to a 32-bit integer in **c**.\n",
    "\n",
    "PyTorch datatypes:\n",
    "\n",
    "- torch.bool\n",
    "- torch.int8\n",
    "- torch.uint8\n",
    "- torch.int16\n",
    "- torch.int32\n",
    "- torch.int64\n",
    "- torch.half\n",
    "- torch.float\n",
    "- torch.double\n",
    "- torch.bfloat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math & Logic with Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic arithmetic with tensors and how tensor interact with simple scalars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.]])\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]])\n",
      "tensor([[4., 4.],\n",
      "        [4., 4.]])\n",
      "tensor([[1.4142, 1.4142],\n",
      "        [1.4142, 1.4142]])\n"
     ]
    }
   ],
   "source": [
    "ones = torch.zeros(2,2) + 1\n",
    "twos = torch.ones(2,2) * 2\n",
    "threes = (torch.ones(2,2) * 7 - 1) / 2\n",
    "fours = twos ** 2\n",
    "sqrt2s = twos ** 0.5\n",
    "\n",
    "print(ones)\n",
    "print(twos)\n",
    "print(threes)\n",
    "print(fours)\n",
    "print(sqrt2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arithmetic operations between tensors and scalars, such as addition, subtraction, multiplication, division, and exponentiation are distributed over every element of the tensor.\n",
    "\n",
    "Operation between tow tensors also behave like you'd intuitively expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.]])\n",
      "tensor([[12., 12.],\n",
      "        [12., 12.]])\n"
     ]
    }
   ],
   "source": [
    "powers2 = twos ** torch.tensor([[1,2], [3,4]])\n",
    "print(powers2)\n",
    "\n",
    "fives = ones + fours\n",
    "print(fives)\n",
    "\n",
    "dozens = threes * fours\n",
    "print(dozens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to note that all of the tensors in the previous code cell were of identical shape. What happens when we try to perform a binary operation on tensor if dissimilar shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors.ipynb Cell 39\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m3\u001b[39m,\u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors.ipynb#X53sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(a \u001b[39m*\u001b[39;49m b)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# The following throws a run-time error. This is intentional.\n",
    "\n",
    "a = torch.rand(2,3)\n",
    "b = torch.rand(3,2)\n",
    "\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general case, you cannot operate on tensors of different shape this way,even in a case like the call above, where the tensor have identical number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Brief: Tensor Broadcasting\n",
    "\n",
    "The exception to the same-shape rule is tensor broadcasting. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2024, 0.5731, 0.7191, 0.4067],\n",
      "        [0.7301, 0.6276, 0.7357, 0.0381]])\n",
      "tensor([[0.4049, 1.1461, 1.4382, 0.8134],\n",
      "        [1.4602, 1.2551, 1.4715, 0.0762]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "rand = torch.rand(2,4)\n",
    "doubled = rand * (torch.ones(1,4)*2)\n",
    "\n",
    "print(rand)\n",
    "print(doubled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is it we get to multiply a 2x4 tensor by a 1x4 tensor?\n",
    "\n",
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. In the example above, the one-row, four-column tensor is multiplied by both rows of the two-row, four-column tensor.\n",
    "\n",
    "![Pytorch Broadcasting](broadcasting.jpg)\n",
    "\n",
    "This is an important operation in Deep Learning. The common example is multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, and returning a tensor of identical shape - just like our (2,4) * (1,4) example above returned a tensor of shape (2,4).\n",
    "\n",
    "The rules of broadcasting are:\n",
    "\n",
    "- Each tensor must have at least one dimension - no empty tensors.\n",
    "- Comparing the dimension sizes of the tow tensors, going from last to first:\n",
    "    - Each dimension must be equal of\n",
    "    - One of the dimension must be of size 1, or\n",
    "    - Dimension does not exist in one of the tensors\n",
    "\n",
    "Tensors of identical shape, of course are trivially \"broadcastable\", as you saw earlier.\n",
    "\n",
    "Here are some examples of situation that honor the above rules and allow broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(     4,  3,  2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]],\n",
      "\n",
      "        [[0.0381, 0.2138],\n",
      "         [0.5395, 0.3686],\n",
      "         [0.4007, 0.7220]]])\n"
     ]
    }
   ],
   "source": [
    "b = a * torch.rand(     3,  2) # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]],\n",
      "\n",
      "        [[0.8217, 0.8217],\n",
      "         [0.2612, 0.2612],\n",
      "         [0.7375, 0.7375]]])\n"
     ]
    }
   ],
   "source": [
    "c = a * torch.rand(     3,  1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]],\n",
      "\n",
      "        [[0.8328, 0.8444],\n",
      "         [0.8328, 0.8444],\n",
      "         [0.8328, 0.8444]]])\n"
     ]
    }
   ],
   "source": [
    "d = a * torch.rand(     1,  2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look closely at the values of each tensor above:\n",
    "\n",
    "- The multiplication operation that created **b** was broadcast over every 'layer' of **a**.\n",
    "- For **c**, the operation was broadcast over ever layer and row of **a** - every 3-element column is identical.\n",
    "- For **d**, we switched it around - now every row is identical, across layers and columns.\n",
    "\n",
    "One example where broadcasting will fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb Cell 42\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#X64sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mones(     \u001b[39m4\u001b[39m,  \u001b[39m3\u001b[39m,  \u001b[39m2\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#X64sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m b \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39;49m torch\u001b[39m.\u001b[39;49mrand(     \u001b[39m4\u001b[39;49m,  \u001b[39m3\u001b[39;49m)  \u001b[39m# dimension must match last-to-first\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#X64sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m c \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrand(     \u001b[39m2\u001b[39m,  \u001b[39m3\u001b[39m)  \u001b[39m# both 3rd & 2nd dims different\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#X64sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m d \u001b[39m=\u001b[39m a \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m0\u001b[39m,)          \u001b[39m# cant broadcast with an empty tensor\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "a = torch.ones(     4,  3,  2)\n",
    "\n",
    "b = a * torch.rand(     4,  3)  # dimension must match last-to-first\n",
    "\n",
    "c = a * torch.rand(     2,  3)  # both 3rd & 2nd dims different\n",
    "\n",
    "d = a * torch.rand(0,)          # cant broadcast with an empty tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Math with Tensor\n",
    "\n",
    "PyTorch tensors have over three hundred operations that can be performed on them.\n",
    "Here a small sample from some of the major categories of operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.0792, 0.3893, 0.6054, 0.3429],\n",
      "        [0.1310, 0.9869, 0.5530, 0.2932]])\n",
      "tensor([[1., -0., -0., -0.],\n",
      "        [1., -0., 1., -0.]])\n",
      "tensor([[ 0., -1., -1., -1.],\n",
      "        [ 0., -1.,  0., -1.]])\n",
      "tensor([[ 0.0792, -0.3893, -0.5000, -0.3429],\n",
      "        [ 0.1310, -0.5000,  0.5000, -0.2932]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vector & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.7016, 0.6826],\n",
      "        [0.9413, 0.4460]])\n",
      "tensor([[2.1048, 2.0479],\n",
      "        [2.8240, 1.3380]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.6835, -0.7299],\n",
      "        [-0.7299,  0.6835]]),\n",
      "S=tensor([4.2305, 0.7013]),\n",
      "V=tensor([[-0.8273,  0.5617],\n",
      "        [-0.5617, -0.8273]]))\n"
     ]
    }
   ],
   "source": [
    "# common functions\n",
    "a = torch.rand(2,4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonemtric fucntion and thier inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:') \n",
    "b = torch.tensor([1,5,11])\n",
    "c = torch.tensor([2,7,10])\n",
    "print(torch.bitwise_xor(b,c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1.,2.],[3.,4.]])\n",
    "e = torch.ones(1,2) # many comparison ops support broadcasting!\n",
    "print(torch.eq(d,e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))         # returns a single-element tensor\n",
    "print(torch.max(d).item())  # extracts the value from the returned tensor\n",
    "print(torch.mean(d))        # average\n",
    "print(torch.std(d))         # standard deviation\n",
    "print(torch.prod(d))        # product of all numbers\n",
    "print(torch.unique(torch.tensor([1,2,1,2,1,2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0.,1.,0.])           # y unit vector\n",
    "m1 = torch.rand(2,2)                    # random matrix\n",
    "m2 = torch.tensor([[3.,0.], [0.,3.]])   # three times identity matrix\n",
    "\n",
    "print('\\nVector & Matrices:')\n",
    "print(torch.cross(v2,v1))               # negative of z unit vector (v1 x v2 == -v2 x v1)\n",
    "print(m1)\n",
    "m3 = torch.matmul(m1,m2)\n",
    "print(m3)                               # 3 times m1\n",
    "print(torch.svd(m3))                    # singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a small sample of more details and the full inventory of math functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering Tensors in Place\n",
    "\n",
    "Most binary operations on tensors will return a third, new tensor. When we say c = a * b (where a and b are tensors), the new tensor c will occupy a region of memory distinct from the other tensors.\n",
    "\n",
    "There are times, though, that you may whish to alter a tensor in place - for example. if you're doing an element-wise computation where you can discard intermediate value. For this, most of the math function have a version with an underscore(_) that will alter a tensor in place.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "\n",
      "b:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('a:')\n",
    "print(a)\n",
    "print(torch.sin(a))     # this operation creates a new tensor in memory\n",
    "print(a)                # a has not changed\n",
    "\n",
    "b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "print('\\nb:')\n",
    "print(b)\n",
    "print(torch.sin_(b))    # note the underscore\n",
    "print(b)                # b has changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For arithmetic operations, there are functions that behave similarly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: \n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[0.9289, 0.6293],\n",
      "        [0.6264, 0.4704]])\n",
      "\n",
      "After adding:\n",
      "tensor([[1.9289, 1.6293],\n",
      "        [1.6264, 1.4704]])\n",
      "tensor([[1.9289, 1.6293],\n",
      "        [1.6264, 1.4704]])\n",
      "tensor([[0.9289, 0.6293],\n",
      "        [0.6264, 0.4704]])\n",
      "\n",
      "After multiplying\n",
      "tensor([[0.8628, 0.3960],\n",
      "        [0.3924, 0.2212]])\n",
      "tensor([[0.8628, 0.3960],\n",
      "        [0.3924, 0.2212]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = torch.rand(2,2)\n",
    "\n",
    "print('Before: ')\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter adding:')\n",
    "print(a.add_(b))\n",
    "print(a)\n",
    "print(b)\n",
    "print('\\nAfter multiplying')\n",
    "print(b.mul_(b))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these in-place arithmetic functions are methods an the torch.Tensor object, not attached to the torch module like many other functions (e.g. torch.sin()). As you can see from a.add_(b), the calling tensor is the one that gets changed in place.\n",
    "\n",
    "There is another option for placing the result of computation in an existing, allocated tensor. Many of the methods and functions we've seen so far - including creation methods! - have an out argument that lets you specify a tensor to receive the output. If the out tensor is the correct shape an dtype, this can happen without a new memory allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0.5694, 0.1489],\n",
      "        [0.8242, 0.4668]])\n",
      "tensor([[0.2138, 0.5395],\n",
      "        [0.3686, 0.4007]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2)\n",
    "b = torch.rand(2,2)\n",
    "c = torch.zeros(2,2)\n",
    "\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a,b, out=c)\n",
    "print(c)                    # contents of c have changed\n",
    "\n",
    "assert c is d               # test c & d are same objects, not just containing equal values\n",
    "assert id(c), old_id        # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2,2, out=c)      # works for creation too!\n",
    "print(c)                    # c has changed again\n",
    "assert id(c), old_id        # still the same object!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying Tensors\n",
    "\n",
    "As with any object in Python, assigning a tensor to a variable makes the variable a label of the tensor, and does not copy  it. For example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1., 561.],\n",
      "        [  1.,   1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a\n",
    "\n",
    "a[0][1] = 561   # we change a ...\n",
    "print(b)        # and b is also altered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what is you wand a separate copy of the data to work on? The clone() method is there for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True],\n",
      "        [True, True]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,2)\n",
    "b = a.clone()\n",
    "\n",
    "assert b is not a       # differen objects in  memory ...\n",
    "print(torch.eq(a,b))    # ... but still with the same contents\n",
    "\n",
    "a[0][1] = 561\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an important thing to be aware of when using clone(). If your source tensor has autograd, enabled then so will the clone. This will be covered more deeply in the video on autograd, but if you want the light version of the details, continue on.\n",
    "\n",
    "In many cases, this will be what you want. For example, if your model has multiple computations paths in its forward() method, and both the original tensor and ist clone contribute to the model's output, then to enable model learning you want autograd turned on for both tensors. If your source tensor has autograd enabled (which it generally will if it's a set of learning weights or derived from a computation involving the weights), then you'll get the result you want.\n",
    "\n",
    "On the other hand, if you're doing a computation where neither the original tensor nor its clone need to track gradients, then as long as the source tensor has autograd turned off, you're good to go.\n",
    "\n",
    "There is a third case, though: imagine you're performing a computation in your model's forward() function, where gradients are turned on for everything by default, but you want to pull out some values mid-stream to generate some metrics. In this case, you don't want the clone copy of your source tensor to track gradients - performance is improved with autograd's history tracking turned off. For this, you can use .detach() method on the source tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2024, 0.5731],\n",
      "        [0.7191, 0.4067]], requires_grad=True)\n",
      "tensor([[0.2024, 0.5731],\n",
      "        [0.7191, 0.4067]], grad_fn=<CloneBackward0>)\n",
      "tensor([[0.2024, 0.5731],\n",
      "        [0.7191, 0.4067]])\n",
      "tensor([[0.2024, 0.5731],\n",
      "        [0.7191, 0.4067]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2,2, requires_grad=True)     #  turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone()\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here ?\n",
    "\n",
    "- We create a with requires_grad=True on. We haven't covered this optional argument yet, but will during the unit on autograd.\n",
    "- When we print a, it informs us that the property requires_grad=True - this means that autograd and computation history tracking are turned on.\n",
    "- We clone a an label it b. When we print b, we can see that it's tracking its computation history - it has inherited a's autograd settings, and added to the computation history.\n",
    "- We clone a into c, but we call detach() first.\n",
    "- Printing c, we see no computation history, and no requires_grad=True.\n",
    "\n",
    "The detach() method detaches the tensor from its computation history. It says \"do whatever comes next as if autograd was off\". It does this without changing a - you can see that when we print a again at the end, it retains its requires_grad=True property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving to GPU\n",
    "\n",
    "One of the major advantage of PyTorch is its robust acceleration on CUDA-compatible Nividia GPUs. (\"CUDA\" stands for Compute Unified Device Architecture, which is Nvidia's platform for parallel computing). So far, everything we've done has been on CPU. How do we move to the faster hardware?\n",
    "\n",
    "First, we should check whether a GPU is available, with the is_available() method.\n",
    "\n",
    "Note: If you do not have a CUDA-compatible GPU and CUDA divers installed, the executable cells in this section will not execute any GPU-related code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have GPU!\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('We have GPU!')\n",
    "else:\n",
    "    print('Sorry, CPU only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've determined that one or more GPUs is available, we need to put out data someplace where the GPU can see it. Your GPU does computation on data in your computers RAM. Your GPU has dedicated memory attached to it. Whenever you want to perform a computation on a device, you must move all the data needed for that computation to memory accessible by that device. (Colloquially, \"moving the data to memory accessible by the GPU\" is shorted to, \"moving the data to the GPU\".)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There multiple ways to get your data onto your target device. Yo may do it at creation time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3344, 0.2640],\n",
      "        [0.2119, 0.0582]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    gpu_rand = torch.rand(2,2,device='cuda')\n",
    "    print(gpu_rand)\n",
    "else:\n",
    "    print('Sorry, GPU only')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, new tensor are created ont the CPU, so we have to specify when we want to create out tensor on the GPU with the optional device argument. You can see when we print the new tensor, PyTorch informs us which device it's on (if it's not on CPU).\n",
    "\n",
    "You can query the number of GPUs with torch.cuda.device_count(). If you have more than one GPU, you can specify them by index: device='cuda:0', device='coda:1', etc.H\n",
    "\n",
    "As a coding practice, specifying our devices everywhere with string constants is pretty fragile. In an ideal world, your code would perform robustly wheter you're on CPU or GPU hardware. You can do this by creating a device handle that can be passed to your tensors instead of a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "tensor([[0.0024, 0.6778],\n",
      "        [0.2441, 0.6812]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    my_device = torch.device('cuda')\n",
    "else:\n",
    "    my_device = torch.device('cpu')\n",
    "print('Device: {}'.format(my_device))\n",
    "\n",
    "x = torch.rand(2,2, device=my_device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an existing tensor living on one device, you can move it to another with the to() method. The following line of code creates a tensor on CPU, and moves it to whichever device handle you acquired in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.rand(2,2)\n",
    "y = y.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know that in order to do computation involving tow or more tensors, all of the tensors must be on the same device. The following code will throw a runtime error, regardless of whether you have a GPU device available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb Cell 75\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#Y150sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#Y150sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m2\u001b[39m,\u001b[39m2\u001b[39m, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jf/Development/quarto/janf-blog/notes/2023-09-09_pytorch_tensors/pytorch_tensors_note.ipynb#Y150sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m z \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39;49m y \u001b[39m# exception will be thrown\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2, device='cpu')\n",
    "y = torch.rand(2,2, device='cuda')\n",
    "\n",
    "z = x + y # exception will be thrown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Tensors Shpares\n",
    "\n",
    "Sometimes, you'll need to change the chape of your tensor. Below. we'll look at a few common cases, and how to handle them.\n",
    "\n",
    "### changing the Number or Dimensions\n",
    "\n",
    "One chase where you might need to change the number of dimension is passing a single instance of input to your model. PyTorch models generally expect batches of input.\n",
    "\n",
    "For example, imagine having a model that works on 3 x 226 x 226 images - a 226-pixel square with 3 color channels. When you load and transform it, you'll get a tensor of shape (3,226,226). Your model, though, is expecting input of shape (N, 3, 226, 226), where N is the number of images in the batch. So how do you make a batch of one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3,226,226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unsqueeze() method adds a dimension of extent 1. unsqueeze(0) adds it as a new zeroth dimension - now you have a batch of one!\n",
    "\n",
    "So if that's unsqueezing? What do we mean by squeezing? We're taking advantage of the fast that any dimension of extent 1 does not chang the number of elements in the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.5731]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1,1,1,1,1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing the example above, let's say the model's output is a 20-element vector for each input. You would then expect the output to have shape (N,20), where N is the number of instances in the input batch. That means that for our single-input batch, we'll get an output of shape(1,20).\n",
    "\n",
    "What if you want to do some non-batched computation with that output - something that's just expecting a 20-element vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.2330, 0.8441, 0.9004, 0.3995, 0.6324, 0.9464, 0.0113, 0.5183, 0.9807,\n",
      "         0.6545, 0.4144, 0.0696, 0.4648, 0.4491, 0.6265, 0.9411, 0.4922, 0.5461,\n",
      "         0.5396, 0.3053]])\n",
      "torch.Size([20])\n",
      "tensor([0.2330, 0.8441, 0.9004, 0.3995, 0.6324, 0.9464, 0.0113, 0.5183, 0.9807,\n",
      "        0.6545, 0.4144, 0.0696, 0.4648, 0.4491, 0.6265, 0.9411, 0.4922, 0.5461,\n",
      "        0.5396, 0.3053])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2,2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the shape that our 2-dimensional tensor is now 1-dimensional, and if you look closely at the output of the cell above you'll see that printing a shows an \"extra\" set of square brackets [] due to having an extra dimension.\n",
    "\n",
    "You may only squeeze() dimension of extent 1. See above where we try to squeeze a dimension of size 2 in c, and get back the same shape we started with. Calls to squeeze() and unsqueeze() can only act on dimensions of extent 1 because to do otherwise would chang the number of elements in the tensor.\n",
    "\n",
    "Another place you might use unsqueeze() is to ease broadcasting. Recall the example above where we had the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.7765, 0.7765],\n",
      "         [0.3534, 0.3534],\n",
      "         [0.7016, 0.7016]],\n",
      "\n",
      "        [[0.7765, 0.7765],\n",
      "         [0.3534, 0.3534],\n",
      "         [0.7016, 0.7016]],\n",
      "\n",
      "        [[0.7765, 0.7765],\n",
      "         [0.3534, 0.3534],\n",
      "         [0.7016, 0.7016]],\n",
      "\n",
      "        [[0.7765, 0.7765],\n",
      "         [0.3534, 0.3534],\n",
      "         [0.7016, 0.7016]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,3,2)\n",
    "\n",
    "c = a * torch.rand(   3,1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The net effect of that was to broadcast the operation over dimensions 0 and 2, causing the random, 3 x 1 tensor to be multiplied element-wise by every 3-element column in a.\n",
    "\n",
    "What if the random vector hat just been 3-element vector? We'd lose the ability to do the broadcast, because the final dimensions would not match up according to the broadcasting rules. unsqueeze() comes to the rescue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.6826, 0.6826],\n",
      "         [0.9413, 0.9413],\n",
      "         [0.4460, 0.4460]],\n",
      "\n",
      "        [[0.6826, 0.6826],\n",
      "         [0.9413, 0.9413],\n",
      "         [0.4460, 0.4460]],\n",
      "\n",
      "        [[0.6826, 0.6826],\n",
      "         [0.9413, 0.9413],\n",
      "         [0.4460, 0.4460]],\n",
      "\n",
      "        [[0.6826, 0.6826],\n",
      "         [0.9413, 0.9413],\n",
      "         [0.4460, 0.4460]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4,3,2)\n",
    "b = torch.rand(   3)    # trying to multiply a * b give a runtime error\n",
    "c = b.unsqueeze(1)      # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squeeze() and unsqueeze() methods also have in-place versions, squeeze_() and unsqueeze_():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "batch_me = torch.rand(3,226,226)\n",
    "print(batch_me.shape)\n",
    "batch_me.unsqueeze_(0)\n",
    "print(batch_me.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you'll want to change the shape of a tensor more radically, while still preserving the number of elements and their contents. One case where this happens is at the interface between a convolutional layer of a model and a linear layer of the model - this is common in image classification models. A convolutional kernal will yield an output tensor of shape features x width x height, but the following linear layer expects a 1-dimensional input. reshape() will do this for you, provided that the dimensions you request yield the same number of elements as the input tensor has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 20, 20])\n",
      "torch.Size([2400])\n",
      "torch.Size([2400])\n"
     ]
    }
   ],
   "source": [
    "output3d = torch.rand(6,20,20)\n",
    "print(output3d.shape)\n",
    "\n",
    "input1d = output3d.reshape(6 * 20 * 20)\n",
    "print(input1d.shape)\n",
    "\n",
    "# can also call it as a method on the torch module:\n",
    "print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: The (6 * 20 * 20) argument in the final line of the cell above is because PyTorch expects a tuple when specifying a tensor shape - but when the shape is the first argument of a method, it lets us cheat and just use a series of integers. Here, we had to add the parentheses and comma to convince the method that is really a one-element tuple.)\n",
    "\n",
    "When it can, reshape() wil return a view on the tensor to be changed - that is a separate tensor object looking at the same underlying region of memory. This is important: That means any change made to the source tensor will be reflected in the view on that tensor, unless you clone() it.\n",
    "\n",
    "There are conditions, beyond the scope of this introduction, where reshape() has to return a tensor carrying a copy of the data. For more information, see the docs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch-Numpy Bridge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the section above on broadcasting, it was mentioned that PyTorch's broadcast semantics are compatible with NumPy's - but the kinship between PyTorch and NumPy goes even deeper than that.\n",
    "\n",
    "If you have existing ML of scientific code with data stored in NumPy ndarrays, you may wish to express that same data as PyTorch tensors, whether to take advantage of PyTorch's GPU acceleration, or its efficient abstractions for building ML models. It's easy to switch between ndarrays and PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numpy_array = np.ones((2,3))\n",
    "print(numpy_array)\n",
    "\n",
    "pytorch_tensor = torch.from_numpy(numpy_array)\n",
    "print(pytorch_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch creates a tensor of the same shape and containing the same data as the NumPy array, going so far as to keep NumPy,s default 64bit float data type.\n",
    "\n",
    "The conversion can just easily go the other way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2587, 0.9526, 0.5603],\n",
      "        [0.8715, 0.9484, 0.7122]])\n",
      "[[0.25874352 0.9526257  0.56027913]\n",
      " [0.87152576 0.9484055  0.712178  ]]\n"
     ]
    }
   ],
   "source": [
    "pytorch_rand = torch.rand(2,3)\n",
    "print(pytorch_rand)\n",
    "\n",
    "numpy_rand = pytorch_rand.numpy()\n",
    "print(numpy_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to know that these converted objects are using the same underlying memory as their source objects, meaning that changes to one are reflected in the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  1.,  1.],\n",
      "        [ 1., 23.,  1.]], dtype=torch.float64)\n",
      "[[ 0.25874352  0.9526257   0.56027913]\n",
      " [ 0.87152576 17.          0.712178  ]]\n"
     ]
    }
   ],
   "source": [
    "numpy_array[1,1] = 23\n",
    "print(pytorch_tensor)\n",
    "\n",
    "pytorch_rand[1,1] = 17\n",
    "print(numpy_rand)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
