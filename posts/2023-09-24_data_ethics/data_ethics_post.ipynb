{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'data ethics'\n",
    "description: 'fastai book chapter 3'\n",
    "author: \"janf\"\n",
    "date: \"2023-09-24\"\n",
    "date-format: iso\n",
    "image: data_ethics_thumbnail.jpg\n",
    "categories: [fastai, deeplearning, self-study]\n",
    "toc: true\n",
    "draft: false\n",
    "title-block-banner: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fastai book chapter 3 - janf - 2023](data_ethics_thumbnail.jpg){fig-align=\"left\" width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This my summary of chapter 3 from the book \"Deep Learning for Coders with fastai & PyTorch\".\n",
    "\n",
    " - questions - question about the chapter\n",
    " - key concepts - summarized key concepts of the chapter\n",
    "\n",
    "::: {.callout-note}\n",
    "## Links \n",
    "\n",
    "- Homepage: [fastai hompage](https://www.fast.ai/)\n",
    "- Online Book: [fastai online book](https://course.fast.ai/Resources/book.html)\n",
    "- Author: [jermey howard](https://jeremy.fast.ai/)\n",
    "- Author: [sylvain gugger](https://sgugger.github.io/)\n",
    "- Co-Author of this chapter: [rachel thomas](https://rachel.fast.ai/about.html)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Questions about the chapter. \n",
    "\n",
    "[Questions - Chapter 3 - Data Ethics](subsite/data_ethics_questions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "No code for this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "\n",
    "Summarized key concepts ot this chapter.\n",
    "\n",
    "## Why do Data Ethics Matter\n",
    "\n",
    "Everybody who is training models needs to consider how their models will be used, and consider how to best ensure that they are used as positively as possible. There are things you can do. And if you don't do them things can go pretty badly. In general there are many negative societal consequences linked to AI an machine learning being observed today (bugs, flawed feedback loops, biases). It'not just a moral burden to consider sometimes there a legal burdens also. As example the first person who was jailed in the Volkswagen diesel scandal was the engineer not the manager. There is no final solution to ensure your work is used the right way. But with the right questions, you can at the very least ensure that the right issues are being considered. And you can so \"no\" to questionable pieces of work if their moral aspects don't align.\n",
    "\n",
    "## Topics in Data Ethics\n",
    "\n",
    "Data ethics is a big field. This summery doesn't cover everything. The following are relevant topics to consider:\n",
    "\n",
    "### Recourse and Accountability\n",
    "\n",
    "In a complex system, it is easy for no one person to feel responsible for outcomes. While this understandable, it does not lead to good results. To hinder big errors you need Recourse and Accountability. An additional reason why recourse is so necessary is that data often contains errors. Mechanism for audits and error correction are crucial and should be considered by by practitioners.\n",
    "\n",
    "### Feedback Loops\n",
    "\n",
    "Feedback loops describe how an algorithm can interact with its environment to make predictions that reinforce action taken in the real world, which lead to predictions even more pronounced in the sam direction. Part of this problem is the driving metric of the algorithm. An algorithm has a metric to optimize, it will do everything it can to optimize their result. This can lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit thees edge cases and feedback loops for their advantage. This behavior of feedback loops and tendencies for optimization can happen. As practitioner you should keep that in your mind and either anticipate a feedback loop and take positive action to break it when it happens.\n",
    "\n",
    "### Bias\n",
    "\n",
    "Bias in machine learning can come from multiple sources. In this section we summarize the types of bias that are most helpful for machine learning projects.\n",
    "\n",
    "- <i>Historical bias</i>\n",
    "    - Historical bias comes from the fact that people are biased, processes are biased, and society is biased. It is fundamental, structural issue with the first step of the data generation process and can exit even give perfect sampling and feature selection.\n",
    "- <i>Measurement bias</i>\n",
    "    - Measurement bias can occur when our model makes mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into the model inappropriately.\n",
    "- <i>Aggregation bias</i>\n",
    "    - Aggregation bias occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so froth.\n",
    "- <i>Representation bias</i>\n",
    "    - When the model emphasize some property of the data as it seemingly has the closet correlation with the prediction, even though that might not be the truth.\n",
    "- <i>Evaluation bias</i>\n",
    "    - Evaluation bias occurs when the benchmark data used for a particular task does not represent the use population. A model is optimized on its training data, but its quality is often measured on benchmarks. These benchmark encourages the development and deployment of models that perform well only on the subset of the data represented by the benchmark data.\n",
    "- <i>Deployment bias</i>\n",
    "    - Deployment bias arises when there is a mismatch between the problem a model is intended to solve and the way in which it is actually used. This often occurs when a system is built an evaluated as if were fully autonomous, while in reality, it operates in a complicated sociotechnical system moderated by human decision-makers.\n",
    "\n",
    "<i>Addressing different types of bias</i>\n",
    "\n",
    "Different types of bias require different approaches for mitigation. All datasets contain bias. There is no such thing as a complete debiased dataset. Many researchers in the field have been converging on a set of proposals to enable better documentation of the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using a particular dataset will not be caught off guard by its biases and limitations.\n",
    "\n",
    "Consider these points when working with machine learning algorithms:\n",
    "\n",
    "- Machines learning can create feedback loops\n",
    "    - Small amounts of bias can rapidly increase exponentially because of feedback loops.\n",
    "- Machine learning can amplify bias\n",
    "    - Human bias can lead to larger amounts of machine learning bias.\n",
    "- Algorithms and humans are used differently\n",
    "    - Human decisions makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice. These examples are given in the list on the next page.\n",
    "- Technology is power\n",
    "    - And with that comes responsibility.\n",
    "\n",
    "## Disinformation\n",
    "\n",
    "Disinformation is not necessarily about getting someone to believe something false, but rather often used to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Is can often contain some seeds of truth, of half-truth taken out of context.\n",
    "\n",
    "With machine learning disinformation can be created cheaper and at a larger scale. Trough autogenerated text machine learning can be used to create coordinated campaigns of inauthentic behavior. For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint.\n",
    "\n",
    "## Addressing Ethical Issues\n",
    "\n",
    "The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them. So what can we do?\n",
    "\n",
    "### Analyze a Project You Are Working On\n",
    "\n",
    "- Consider the ethical implication of your work. Question to ask?\n",
    "    - Should we even be doing this?\n",
    "    - What bias is in the data?\n",
    "    - Can the code and data be audited?\n",
    "    - What processes are in place to handle appeals or mistakes?\n",
    "\n",
    "### Processes to Implement\n",
    "\n",
    "- Concrete practices to implement to proactively search for ethical risks.\n",
    "    - Expanding the ethical circle to include the perspectives of a variety of stakeholder.\n",
    "    - Consult interests, desires, skills, experiences, and values that maybe were simply assumed.\n",
    "    - Consider all stakeholders and their interests. Also the individuals that will be indirectly affected by our products.\n",
    "    - Also consider terrible people. Who might use this product in ways we didn't expected.\n",
    "\n",
    "### The Power of Diversity\n",
    "\n",
    "- When everybody on a team has similar backgrounds, they are likely to have similar blind spots around ethical risks. Diversity can lead to problems being identified earlier, and a wider range of solutions being considered.\n",
    "\n",
    "### Fairness, Accountability, and Transparency\n",
    "\n",
    "- Treat fairness as a central concern rather than an afterthought. Don't sidestep deeper questions about fairness, accountability and transparency.\n",
    "\n",
    "### Regulation, Rights and Policy\n",
    "\n",
    "- Policies are an appropriate tool for addressing data ethics issues when is likely that design fixes, self regulation and technical approaches to addressing problems, involving ethical uses of Machine Learning are not working. While such measures can be useful, they will not be sufficient to address the underlying problems that have led to our current state. For example, as long as it is incredibly profitable to create addictive technology, companies will continue to do so, regardless of whether this has the side effect of promoting conspiracy theories and polluting our information ecosystem. While individual designers may try to tweak product designs, we will not see substantial changes until the underlying profit incentives changes. Because of the above it is almost certain that policies will have to be created by government to address these issues.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
