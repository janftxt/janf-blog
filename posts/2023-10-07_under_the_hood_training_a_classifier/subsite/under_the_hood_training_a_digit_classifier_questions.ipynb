{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions - Chapter 4\n",
    "::: {.callout-warning}\n",
    "## Back to blog post\n",
    "[fastai book chapter 4](../under_the_hood_training_a_digit_classifier_post.ipynb)\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "## Links\n",
    "- Source: [Fastbook Chapter 4 questionnaire solutions (wiki)](https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "How is a grayscale image represented on a computer? How about a color image?\n",
    "</b>\n",
    "\n",
    "Images are represented by arrays with pixel values representing the content of the image. For greyscale images, a 2-dimentional array is used with the pixeles representing the grayscale values, with a range of 256 integers. A value of 0 would represent white and a value of 255 represent black, and different shades of greyscale in between. For color images, three color channels (red,green,blue) are typicall used, with a separate 256-range 2D array used for each channel. A pixel of 0 again represent white, with 255 representing solid red, green or blue. The 2D arrays from a final 3D array (rank 3 tensor) representing the color image.\n",
    "\n",
    "<b>\n",
    "How are the files an folders in the MNIST_SAMPLE dataset structured? Why?\n",
    "</b>\n",
    "\n",
    "There are two subfolders, train and valid, the former contains the data for modeling training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves tow purposes: a.) to report a human interpretable metric such as accuracy (in contract to the often abstract loss functions used for training), b.) to facilitate the detection of overfitting by evaluating the model on da dataset it hasn't been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so an the validation set). Of course, every practitioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifying comparing results between implementations/publications.\n",
    "\n",
    "<b>\n",
    "Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "</b>\n",
    "\n",
    "In the pixel similarity approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3's from images of 7's. We define the archetypical 3 as the pixel-wise mean values of all 3's in the training set. Analog for the 7's. You can visualize the tow archetypes and see that they are in fact blurred version of the number they represent. In order to tell if previously unseen image is a 3 or a 7, we calculate its distance to the tow archetypes (here: mean pixel-wise absolute difference). We say the new images are a 3 if the distance to the archetypical 3 is lower that tow the archetypical 7.\n",
    "\n",
    "<b>\n",
    "What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "</b>\n",
    "\n",
    "List (arrays in other programming languages) are often generated using a for-loop. A list comprehension in Python is condensing the creation of list using a for-loop into a single expression. List comprehension will also  often include if clauses for filtering.\n",
    "\n",
    "```python\n",
    "lst_in = range(10)\n",
    "lst_out = [2*el for el in lst_in if el%2==1]\n",
    "```\n",
    "\n",
    "```python\n",
    "list = []\n",
    "for el in lst_in:\n",
    "    if el%2==1:\n",
    "        lst_out.append(2*el)\n",
    "        \n",
    "lst_out\n",
    "```\n",
    "\n",
    "<b>\n",
    "What is a rank-3 tensor?\n",
    "</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(list(range(1,10))).view(3,3);\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.],\n",
      "        [14., 16., 18.]])\n"
     ]
    }
   ],
   "source": [
    "b = 2 * a\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 12.],\n",
       "        [16., 18.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "What is broadcasting?\n",
    "</b>\n",
    "\n",
    "Sientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTorch, tensor with saller rank are expanded to have the same size as the larger tensor. In this way, operations can be performed between tensor with different rank.\n",
    "\n",
    "<b>\n",
    "Are metrics generally calculated using the trining set or the validation set? Why?\n",
    "</b>\n",
    "\n",
    "Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data.\n",
    "\n",
    "<b>\n",
    "What is SGD?\n",
    "</b>\n",
    "\n",
    "SGD, or stochastic gradient decent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the prediction and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides and indication of how that loss function changes in the parameter space, which we use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\n",
    "\n",
    "<b>\n",
    "Why does SGD use mini-batches?\n",
    "</b>\n",
    "\n",
    "We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole dataset due to compute limitations and time constraints. If we iterated through each data point, however, the gradients will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using a mini-batches are also more computationally efficient that single items on a GPU.\n",
    "\n",
    "<b>\n",
    "What are the seven steps in SGD for machine learning?\n",
    "</b>\n",
    "\n",
    "1. Initialize the weights.\n",
    "2. Use the weights to predict.\n",
    "3. Based on the prediction, calculate the loss (how good the model is).\n",
    "4. Calculate the gradient which measures for each weight how changing that weight would change the loss.\n",
    "5. Step all the weights based on that calculation.\n",
    "6. Go back to step 2.\n",
    "7. Iterate until you decide to stop the training process (for instance, because the model is good enough or you don't want to wait any longer).\n",
    "\n",
    "<b>\n",
    "How do we initialize the weights in a model?\n",
    "</b>\n",
    "\n",
    "We initialize the parameters to random values.\n",
    "\n",
    "<b>\n",
    "What a loss?\n",
    "</b>\n",
    "\n",
    "The loss gives a number of how good the effectiveness of a current weight assignment is in terms of actual performance. The loss function should return a number that is small if the performance of the model is good. (the standard approach ist to teat a small loss as good and a large loss as bad)\n",
    "\n",
    "<b>\n",
    "Why can't we use a high learning rate?\n",
    "</b>\n",
    "\n",
    "The size of the step we take when applying SGD to update the parameters of the model.The learning rate is often a number between 0.001 and 0.1, although is could be anything. Often people select a learning rate just by typing a few, and finding which results in the best model after training.\n",
    "\n",
    "<b>\n",
    "What is a gradient?\n",
    "</b>\n",
    "\n",
    "The derivative of the loss with respect to some parameter of the model. The gradients are calculated in the backpropagation (this function could also be called calculate_gradients).\n",
    "The gradients tell us only the slope of the function, they don't tell exactly how far to adjust the parameters. But they give us some idea of how far: if the slope is very large, that may suggest that we have more adjustments to do, whereas it the slope is very small, that may suggest that we are close to the optimal value.\n",
    "\n",
    "<b>\n",
    "Do you need to know how to calculate gradients yourself?\n",
    "</b>\n",
    "\n",
    "Manual calculation of the gradients are not required, as deep learning libraries will aromatically calculate the gradients for you. This feature is known as automatic differentiation. In PyTorch, if requires_grad=True, the gradients can be returned by calling the backward method (backpropagation).\n",
    "\n",
    "<b>\n",
    "Why can't we use accuracy as a loss function?\n",
    "</b>\n",
    "\n",
    "A loss function needs to change as the weights are being adjusted. Accuracy only changes if the predictions of the model change. So if there are slight changes to the model, say it improves confidence in a prediction, but does not change the prediction, the accuracy will still not change. Therefore, the gradients will be zero everywhere except when the actual predictions change. The model therefore cannot learn form the gradients equal to zero, and the model's weights will not update and will not train. A good loss function gives a slightly better loss when the model gives slightly better prediction. Slightly better predictions mean if the model is more confident about the correct prediction. For example, predicting 0.9 vs 0.7 for a probability the loss function needs to reflect that.\n",
    "\n",
    "<b>\n",
    "Draw the sigmoid function. What is pecial about its shape?\n",
    "</b>\n",
    "\n",
    "\n",
    "\n",
    "<b>\n",
    "What is the difference between a loss function and a metric?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What is the function to calculate new weights using a learning rate?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What does the DataLoader class do?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "Write pseudocode showing the basic steps taken in each epoch for SGD.\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "Create a function that, if passed two arguments [1,2,3,4] and 'abcd' returns [(1,'a'),(2,'b'),(3,'c'),(4,'d')]. What is special about that output data structure?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What does view do in PyTorch?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What are the bias parameters in a neural network? Why do we need them?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What does the @ operator do in Python?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What does the backward method do?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "Why do we have to zero the gradients?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What information do we have to pass to Learner?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "Show Python or pseudocode for the basic steps of a training loop.\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What is ReLU? Draw a plot of it for values from -2 to +2.\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What is a activation function?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "What's the difference between F.rulu and nn.ReLUJ?\n",
    "</b>\n",
    "\n",
    "<b>\n",
    "The universal approximation theorem shows that any function can be approximated as closely as needed using just one nonlinearity. So why do we normally use more?\n",
    "</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
