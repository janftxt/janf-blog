{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions - Chapter 4\n",
    "::: {.callout-warning}\n",
    "## Back to blog post\n",
    "[fastai book chapter 4](../under_the_hood_training_a_digit_classifier_post.ipynb)\n",
    ":::\n",
    "\n",
    "::: {.callout-note}\n",
    "## Links\n",
    "- Source: [Fastbook Chapter 4 questionnaire solutions (wiki)](https://forums.fast.ai/t/fastbook-chapter-3-questionnaire-solutions-wiki/68042)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "How is a grayscale image represented on a computer? How about a color image?\n",
    "</b>\n",
    "\n",
    "Images are represented by arrays with pixel values representing the content of the image. For greyscale images, a 2-dimentional array is used with the pixeles representing the grayscale values, with a range of 256 integers. A value of 0 would represent white and a value of 255 represent black, and different shades of greyscale in between. For color images, three color channels (red,green,blue) are typicall used, with a separate 256-range 2D array used for each channel. A pixel of 0 again represent white, with 255 representing solid red, green or blue. The 2D arrays from a final 3D array (rank 3 tensor) representing the color image.\n",
    "\n",
    "<b>\n",
    "How are the files an folders in the MNIST_SAMPLE dataset structured? Why?\n",
    "</b>\n",
    "\n",
    "There are two subfolders, train and valid, the former contains the data for modeling training, the latter contains the data for validating model performance after each training step. Evaluating the model on the validation set serves tow purposes: a.) to report a human interpretable metric such as accuracy (in contract to the often abstract loss functions used for training), b.) to facilitate the detection of overfitting by evaluating the model on da dataset it hasn't been trained on (in short, an overfitting model performs increasingly well on the training set but decreasingly so an the validation set). Of course, every practitioner could generate their own train/validation-split of the data. Public datasets are usually pre-split to simplifying comparing results between implementations/publications.\n",
    "\n",
    "<b>\n",
    "Explain how the \"pixel similarity\" approach to classifying digits works.\n",
    "</b>\n",
    "\n",
    "In the pixel similarity approach, we generate an archetype for each class we want to identify. In our case, we want to distinguish images of 3's from images of 7's. We define the archetypical 3 as the pixel-wise mean values of all 3's in the training set. Analog for the 7's. You can visualize the tow archetypes and see that they are in fact blurred version of the number they represent. In order to tell if previously unseen image is a 3 or a 7, we calculate its distance to the tow archetypes (here: mean pixel-wise absolute difference). We say the new images are a 3 if the distance to the archetypical 3 is lower that tow the archetypical 7.\n",
    "\n",
    "<b>\n",
    "What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.\n",
    "</b>\n",
    "\n",
    "List (arrays in other programming languages) are often generated using a for-loop. A list comprehension in Python is condensing the creation of list using a for-loop into a single expression. List comprehension will also  often include if clauses for filtering.\n",
    "\n",
    "```python\n",
    "lst_in = range(10)\n",
    "lst_out = [2*el for el in lst_in if el%2==1]\n",
    "```\n",
    "\n",
    "```python\n",
    "list = []\n",
    "for el in lst_in:\n",
    "    if el%2==1:\n",
    "        lst_out.append(2*el)\n",
    "        \n",
    "lst_out\n",
    "```\n",
    "\n",
    "<b>\n",
    "What is a rank-3 tensor?\n",
    "</b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.],\n",
      "        [7., 8., 9.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(list(range(1,10))).view(3,3);\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  4.,  6.],\n",
      "        [ 8., 10., 12.],\n",
      "        [14., 16., 18.]])\n"
     ]
    }
   ],
   "source": [
    "b = 2 * a\n",
    "\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 12.],\n",
       "        [16., 18.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[1:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>\n",
    "What is broadcasting?\n",
    "</b>\n",
    "\n",
    "Sientific/numerical Python packages like NumPy and PyTorch will often implement broadcasting that often makes code easier to write. In the case of PyTroch, tensor with maller rank are expanded to have the same size as the larger tensor. In this way, operations can be performed between tensor with different rank.\n",
    "\n",
    "<b>\n",
    "Are metrics generally calculated using the trining set or the validation set? Why?\n",
    "</b>\n",
    "\n",
    "Metrics are generally calculated on a validation set. As the validation set is unseen data for the model, evaluating the metrics on the validation set is better in order to determine if there is any overfitting and how well the model might generalize if given similar data.\n",
    "\n",
    "<b>\n",
    "What is SGD?\n",
    "</b>\n",
    "\n",
    "SGD, or stochastic gradient decent, is an optimization algorithm. Specifically, SGD is an algorithm that will update the parameters of a model in order to minimize a given loss function that was evaluated on the prediction and target. The key idea behind SGD (and many optimization algorithms, for that matter) is that the gradient of the loss function provides and indication of how that loss function changes in the parameter space, which we use to determine how best to update the parameters in order to minimize the loss function. This is what SGD does.\n",
    "\n",
    "<b>\n",
    "Why does SGD use mini-batches?\n",
    "</b>\n",
    "\n",
    "We need to calculate our loss function (and our gradient) on one or more data points. We cannot calculate on the whole dataset due to compute limitations and time constraints. If we iterated through each data point, however, the gradients will be unstable and imprecise, and is not suitable for training. As a compromise, we calculate the average loss for a small subset of the dataset at a time. This subset is called a mini-batch. Using a mini-batches are also more computationally efficient that single items on a GPU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
