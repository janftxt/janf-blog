{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'karpathy micrograd'\n",
    "description: 'a tiny scalar-valued autograd engine and a neural net libarary on top of it with PyTorch-like API'\n",
    "author: \"janf\"\n",
    "date: \"2024-03-28\"\n",
    "date-format: iso\n",
    "image: micrograd_thumbnail.jpg\n",
    "categories: [karpathy, deeplearning, self-study]\n",
    "toc: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![karpathy micrograd - janf - 2024](micrograd_thumbnail.jpg){fig-align=\"left\" width=50%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "Some notes to Micrograd from Andrej Karpathy.\n",
    "\n",
    "::: {.callout-note}\n",
    "## Links \n",
    "\n",
    "- Andrej Karpathy video: [Micrograd Video](https://youtu.be/VMj-3S1tku0)\n",
    "- Andrej Karpathy code from the video: [Code from the Video](https://github.com/karpathy/nn-zero-to-hero/tree/master/lectures/micrograd)\n",
    "- Andrej Karpathy micrograd implementation: [Micrograd implementation](https://github.com/karpathy/micrograd)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micrograd\n",
    "\n",
    "Micrograd is a compact autograd-/deep-learning- engine. It implements backpropagation to calculate the gradients of a mathematical function. Furthermore it provides Classes for Neurons, Layers and MultiLayerPerceptrons to build out neural network (which is also only mathematical function) to do backpropagation on it.\n",
    "\n",
    "\n",
    "## Autograd\n",
    "\n",
    "Autograd (Automatic Gradient) records a graph recording all of the operations that created the data as you execute operations, giving you a directed graph. The leafs are the input tensors and roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "## Derivative\n",
    "\n",
    "For Micrograd (and deep-learning) it is important to understand what a derivative is and what information it outputs.\n",
    "\n",
    "The derivative of a function describes the function's instantaneous rate of change at at certain point. Another common interpretation is that the derivative gives us the slope of the line tangent to the function's graph at that point.\n",
    "\n",
    "![differentiability of a function](derivative.svg)\n",
    "\n",
    "Example of derivative of a simple function:\n",
    "\n",
    "f = 3x**2 - 4x + 5</br>\n",
    "h = 0.001</br>\n",
    "x = 3.0\n",
    "\n",
    "how_much_the_function_responded = f(x+h) - f(x)</br>\n",
    "slope (rise over run) = (f(x+h) - f(x))/h\n",
    "\n",
    "In a point you interested, you slightly bump up that point (increase it by a small number h). How does the function respond with that sensitivity does it respond what is the slope at that point does the function go up or down and how much.\n",
    "\n",
    "## Value Object\n",
    "\n",
    "Micrograd core object is the value object. It has the following parameters:\n",
    "\n",
    "- value (the current value data)\n",
    "- gradient (derivative of the value concerning the input of the function)\n",
    "- _backward (backward function that calculates via chaining the outputs gradients into the local gradients)\n",
    "- _prev (children that produced the value of this value object)\n",
    "- _op (operation of sign as string for printing)\n",
    "\n",
    "The value object has methods to perform calculations with different operations. Graphviz library was used to draw out the graph of the mathematical functions of the values, gradients and operations used on the value objects.\n",
    "\n",
    "![differentiability of a function](derivative.svg)\n",
    "\n",
    "## Forward pass\n",
    "\n",
    "Computes an output for a neuralnetwork. The forward pass takes inputs and weights to calculate the output.\n",
    "\n",
    "With the implementation of the value objet and putting the mathematical expression together the forward pass i complete and the output is computed.\n",
    "\n",
    "```python\n",
    "a = Value(2.0, label = 'a')\n",
    "b = Value(-3.0, label = 'b')\n",
    "e =  a * b; e.label = 'e'\n",
    "## e = -6\n",
    "c = Value(10.0, label = 'c')\n",
    "d = e + c; d.label = 'd'\n",
    "## d = 4\n",
    "f = Value(-2.0, label = 'f')\n",
    "L = d * f; L.label = 'L'\n",
    "## L = -8\n",
    "```\n",
    "![visual example of a mathematical expression with value objects](forwardpass_valueobject.png){width=100%}\n",
    "\n",
    "## Chainrule\n",
    "\n",
    "If a variable z depends on the variable y, which itself depends on the variable x (that is, y and z are dependent variables), then z depends on x as well, via the intermediate variable y.\n",
    "\n",
    "In this case the chain rule is expressed as \n",
    "\n",
    "dz/dx = dz/dy * dy/dx\n",
    "\n",
    "Intuitive explanation: The chain rule states that knowing the instantaneous rate of change of z relative to y and that of y relative to x allows one to calculate the instantaneous rate of change of z relative to x as the product of the two rates of change.\n",
    "\n",
    "\"If a car travels twice as fast as a bicycle and the bicycle is four times as fast as a walking man, the car travels 2 x 4 = 8 times as fast as the man.\"\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "## Operations\n",
    "\n",
    "## Neuron\n",
    "\n",
    "## Layer and MLP (Multi-Layer-Perceptron)\n",
    "\n",
    "## Loss\n",
    "\n",
    "## Optimizer (Performing gradient descent)\n",
    "\n",
    "# Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
